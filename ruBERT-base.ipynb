{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2436f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import pymorphy2\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from functools import lru_cache\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a54c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6968e0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>target</th>\n",
       "      <th>word_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Екатеринбург, ул. Московская / ул. Волгоградск...</td>\n",
       "      <td>Московский квартал</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Жилой комплекс</td>\n",
       "      <td>Московский квартал 2.\\nШумно : летом по ночам ...</td>\n",
       "      <td>квартал лето ночь гонка стройка окно этаж райо...</td>\n",
       "      <td>1</td>\n",
       "      <td>московский квартал \\nшумно лето ночь дикий гон...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Московская область, Электросталь, проспект Лен...</td>\n",
       "      <td>Продукты Ермолино</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Магазин продуктов;Продукты глубокой заморозки;...</td>\n",
       "      <td>Замечательная сеть магазинов в общем, хороший ...</td>\n",
       "      <td>сеть магазин ассортимент цена главное качество...</td>\n",
       "      <td>2</td>\n",
       "      <td>замечательный сеть магазин общий хороший ассор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Краснодар, Прикубанский внутригородской округ,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Фитнес-клуб</td>\n",
       "      <td>Не знаю смутят ли кого-то данные правила, но я...</td>\n",
       "      <td>правило шкаф замочка отпечаток палец дичь подп...</td>\n",
       "      <td>0</td>\n",
       "      <td>знать смутить дать правило удивить хотеть твой...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Санкт-Петербург, проспект Энгельса, 111, корп. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Пункт проката;Прокат велосипедов;Сапсёрфинг</td>\n",
       "      <td>Хорошие условия аренды. \\nДружелюбный персонал...</td>\n",
       "      <td>условие аренда персонал ботинок крепление сноу...</td>\n",
       "      <td>2</td>\n",
       "      <td>хороший условие аренда \\nдружелюбный персонал ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Тверь, Волоколамский проспект, 39</td>\n",
       "      <td>Студия Beauty Brow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Салон красоты;Визажисты, стилисты;Салон бровей...</td>\n",
       "      <td>Топ мастер Ангелина топ во всех смыслах ) Немн...</td>\n",
       "      <td>топ топ смысл посещение бровь ресница итог раб...</td>\n",
       "      <td>2</td>\n",
       "      <td>мастер ангелина весь смысл немного волноваться...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address                name  \\\n",
       "0  Екатеринбург, ул. Московская / ул. Волгоградск...  Московский квартал   \n",
       "1  Московская область, Электросталь, проспект Лен...   Продукты Ермолино   \n",
       "2  Краснодар, Прикубанский внутригородской округ,...             LimeFit   \n",
       "3   Санкт-Петербург, проспект Энгельса, 111, корп. 1        Snow-Express   \n",
       "4                  Тверь, Волоколамский проспект, 39  Студия Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0     3.0                                     Жилой комплекс   \n",
       "1     5.0  Магазин продуктов;Продукты глубокой заморозки;...   \n",
       "2     1.0                                        Фитнес-клуб   \n",
       "3     4.0        Пункт проката;Прокат велосипедов;Сапсёрфинг   \n",
       "4     5.0  Салон красоты;Визажисты, стилисты;Салон бровей...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Московский квартал 2.\\nШумно : летом по ночам ...   \n",
       "1  Замечательная сеть магазинов в общем, хороший ...   \n",
       "2  Не знаю смутят ли кого-то данные правила, но я...   \n",
       "3  Хорошие условия аренды. \\nДружелюбный персонал...   \n",
       "4  Топ мастер Ангелина топ во всех смыслах ) Немн...   \n",
       "\n",
       "                                             aspects  target  \\\n",
       "0  квартал лето ночь гонка стройка окно этаж райо...       1   \n",
       "1  сеть магазин ассортимент цена главное качество...       2   \n",
       "2  правило шкаф замочка отпечаток палец дичь подп...       0   \n",
       "3  условие аренда персонал ботинок крепление сноу...       2   \n",
       "4  топ топ смысл посещение бровь ресница итог раб...       2   \n",
       "\n",
       "                                            word_pre  \n",
       "0  московский квартал \\nшумно лето ночь дикий гон...  \n",
       "1  замечательный сеть магазин общий хороший ассор...  \n",
       "2  знать смутить дать правило удивить хотеть твой...  \n",
       "3  хороший условие аренда \\nдружелюбный персонал ...  \n",
       "4  мастер ангелина весь смысл немного волноваться...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5654d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\", num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8558c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_with_rubric'] = df['rubrics'] + ': ' + df['text']\n",
    "df = df.dropna(subset=['text_with_rubric'])\n",
    "def cleann_text(row):\n",
    "    return re.sub(r'\\\\|n', '', row)\n",
    "\n",
    "\n",
    "df['text_with_rubric'] = df['text_with_rubric'].apply(lambda x: cleann_text(x))\n",
    "df_data = df[['text_with_rubric', 'target']]\n",
    "data = df_data.rename(columns={\"text_with_rubric\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b65afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118221,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = data['text'].values\n",
    "labels = data['target'].values\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9df892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Жилой комплекс: Московский квартал 2.Шумно : летом по ночам дикие гонки. Грязно : кругом стройки, невозможно открыть окна (16 этаж! ), вечно по району летает мусор. Детские площадки убогие, на большой площади однотипные конструкции. Очень дорогая коммуналка. Часто срабатывает пожарная сигнализация. Жильцы уже не реагируют. В это время, обычно около часа, не работают лифты. Из плюсов - отличная планировка квартир ( Московская 194 ), на мой взгляд. Ремонт от застройщика на 3-. Окна вообще жуть - вместо вентиляции. По соотношению цена/качество - 3.\n",
      "Tokenized:  ['Жил', '##ой', 'комплекс', ':', 'Московский', 'квартал', '2', '.', 'Шум', '##но', ':', 'летом', 'по', 'ночам', 'дикие', 'гонки', '.', 'Гряз', '##но', ':', 'кругом', 'стройки', ',', 'невозможно', 'открыть', 'окна', '(', '16', 'этаж', '!', ')', ',', 'вечно', 'по', 'району', 'летает', 'мусор', '.', 'Детские', 'площадки', 'убогие', ',', 'на', 'большой', 'площади', 'однотип', '##ные', 'конструкции', '.', 'Очень', 'дорогая', 'коммуналка', '.', 'Часто', 'срабатывает', 'пожарная', 'сигнализация', '.', 'Жил', '##ь', '##цы', 'уже', 'не', 'реагируют', '.', 'В', 'это', 'время', ',', 'обычно', 'около', 'часа', ',', 'не', 'работают', 'лифты', '.', 'Из', 'плюсов', '-', 'отличная', 'плани', '##ровка', 'квартир', '(', 'Московская', '194', ')', ',', 'на', 'мой', 'взгляд', '.', 'Ремонт', 'от', 'застройщика', 'на', '3', '-', '.', 'Окна', 'вообще', 'жуть', '-', 'вместо', 'вентиляции', '.', 'По', 'соотношению', 'цена', '/', 'качество', '-', '3', '.']\n",
      "Token IDs:  [22241, 1519, 9629, 156, 17283, 30807, 140, 132, 22460, 793, 156, 7885, 797, 23465, 29068, 23599, 132, 37684, 793, 156, 16490, 44923, 128, 3034, 10336, 10013, 120, 2946, 20895, 106, 122, 128, 11024, 797, 18187, 18984, 10378, 132, 89936, 20518, 64384, 128, 801, 2902, 9378, 62029, 906, 13722, 132, 3065, 14613, 74463, 132, 15958, 42258, 83366, 74622, 132, 22241, 357, 1249, 1034, 802, 37357, 132, 436, 838, 1210, 128, 2323, 2760, 4323, 128, 802, 4842, 64005, 132, 1724, 23494, 130, 14309, 4512, 7819, 22194, 120, 25613, 3466, 122, 128, 801, 1829, 4150, 132, 62482, 847, 74333, 801, 142, 130, 132, 84358, 1264, 44977, 130, 2792, 52955, 132, 914, 70722, 3628, 134, 5621, 130, 142, 132]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d09261",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 512,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d8210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118221, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3807046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88,665 training samples\n",
      "29,556 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54535b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf1938d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ee7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19cf5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b73c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1)\n",
    "    f1 = f1_score(labels, pred_flat, average='weighted')  # Можно выбрать другой вариант усреднения: 'micro', 'macro', 'samples', 'weighted'\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a072c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42d6b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7fb989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "  Batch    40  of  5,542.    Elapsed: 0:01:20.\n",
      "  Batch    80  of  5,542.    Elapsed: 0:02:37.\n",
      "  Batch   120  of  5,542.    Elapsed: 0:03:54.\n",
      "  Batch   160  of  5,542.    Elapsed: 0:05:11.\n",
      "  Batch   200  of  5,542.    Elapsed: 0:06:28.\n",
      "  Batch   240  of  5,542.    Elapsed: 0:07:45.\n",
      "  Batch   280  of  5,542.    Elapsed: 0:09:02.\n",
      "  Batch   320  of  5,542.    Elapsed: 0:10:19.\n",
      "  Batch   360  of  5,542.    Elapsed: 0:11:37.\n",
      "  Batch   400  of  5,542.    Elapsed: 0:12:54.\n",
      "  Batch   440  of  5,542.    Elapsed: 0:14:11.\n",
      "  Batch   480  of  5,542.    Elapsed: 0:15:28.\n",
      "  Batch   520  of  5,542.    Elapsed: 0:16:58.\n",
      "  Batch   560  of  5,542.    Elapsed: 0:18:15.\n",
      "  Batch   600  of  5,542.    Elapsed: 0:19:33.\n",
      "  Batch   640  of  5,542.    Elapsed: 0:20:50.\n",
      "  Batch   680  of  5,542.    Elapsed: 0:22:07.\n",
      "  Batch   720  of  5,542.    Elapsed: 0:23:24.\n",
      "  Batch   760  of  5,542.    Elapsed: 0:24:41.\n",
      "  Batch   800  of  5,542.    Elapsed: 0:25:58.\n",
      "  Batch   840  of  5,542.    Elapsed: 0:27:16.\n",
      "  Batch   880  of  5,542.    Elapsed: 0:28:33.\n",
      "  Batch   920  of  5,542.    Elapsed: 0:29:50.\n",
      "  Batch   960  of  5,542.    Elapsed: 0:31:07.\n",
      "  Batch 1,000  of  5,542.    Elapsed: 0:32:24.\n",
      "  Batch 1,040  of  5,542.    Elapsed: 0:33:41.\n",
      "  Batch 1,080  of  5,542.    Elapsed: 0:34:58.\n",
      "  Batch 1,120  of  5,542.    Elapsed: 0:36:15.\n",
      "  Batch 1,160  of  5,542.    Elapsed: 0:37:33.\n",
      "  Batch 1,200  of  5,542.    Elapsed: 0:38:50.\n",
      "  Batch 1,240  of  5,542.    Elapsed: 0:40:07.\n",
      "  Batch 1,280  of  5,542.    Elapsed: 0:41:24.\n",
      "  Batch 1,320  of  5,542.    Elapsed: 0:42:41.\n",
      "  Batch 1,360  of  5,542.    Elapsed: 0:43:58.\n",
      "  Batch 1,400  of  5,542.    Elapsed: 0:45:15.\n",
      "  Batch 1,440  of  5,542.    Elapsed: 0:46:32.\n",
      "  Batch 1,480  of  5,542.    Elapsed: 0:47:49.\n",
      "  Batch 1,520  of  5,542.    Elapsed: 0:49:07.\n",
      "  Batch 1,560  of  5,542.    Elapsed: 0:50:23.\n",
      "  Batch 1,600  of  5,542.    Elapsed: 0:51:41.\n",
      "  Batch 1,640  of  5,542.    Elapsed: 0:52:58.\n",
      "  Batch 1,680  of  5,542.    Elapsed: 0:54:15.\n",
      "  Batch 1,720  of  5,542.    Elapsed: 0:55:32.\n",
      "  Batch 1,760  of  5,542.    Elapsed: 0:56:49.\n",
      "  Batch 1,800  of  5,542.    Elapsed: 0:58:06.\n",
      "  Batch 1,840  of  5,542.    Elapsed: 0:59:23.\n",
      "  Batch 1,880  of  5,542.    Elapsed: 1:00:40.\n",
      "  Batch 1,920  of  5,542.    Elapsed: 1:01:58.\n",
      "  Batch 1,960  of  5,542.    Elapsed: 1:03:15.\n",
      "  Batch 2,000  of  5,542.    Elapsed: 1:04:32.\n",
      "  Batch 2,040  of  5,542.    Elapsed: 1:05:49.\n",
      "  Batch 2,080  of  5,542.    Elapsed: 1:07:06.\n",
      "  Batch 2,120  of  5,542.    Elapsed: 1:08:23.\n",
      "  Batch 2,160  of  5,542.    Elapsed: 1:09:40.\n",
      "  Batch 2,200  of  5,542.    Elapsed: 1:10:57.\n",
      "  Batch 2,240  of  5,542.    Elapsed: 1:12:14.\n",
      "  Batch 2,280  of  5,542.    Elapsed: 1:13:31.\n",
      "  Batch 2,320  of  5,542.    Elapsed: 1:14:48.\n",
      "  Batch 2,360  of  5,542.    Elapsed: 1:16:05.\n",
      "  Batch 2,400  of  5,542.    Elapsed: 1:17:23.\n",
      "  Batch 2,440  of  5,542.    Elapsed: 1:18:40.\n",
      "  Batch 2,480  of  5,542.    Elapsed: 1:19:57.\n",
      "  Batch 2,520  of  5,542.    Elapsed: 1:21:14.\n",
      "  Batch 2,560  of  5,542.    Elapsed: 1:22:31.\n",
      "  Batch 2,600  of  5,542.    Elapsed: 1:23:48.\n",
      "  Batch 2,640  of  5,542.    Elapsed: 1:25:05.\n",
      "  Batch 2,680  of  5,542.    Elapsed: 1:26:22.\n",
      "  Batch 2,720  of  5,542.    Elapsed: 1:27:39.\n",
      "  Batch 2,760  of  5,542.    Elapsed: 1:28:56.\n",
      "  Batch 2,800  of  5,542.    Elapsed: 1:30:13.\n",
      "  Batch 2,840  of  5,542.    Elapsed: 1:31:30.\n",
      "  Batch 2,880  of  5,542.    Elapsed: 1:32:47.\n",
      "  Batch 2,920  of  5,542.    Elapsed: 1:34:04.\n",
      "  Batch 2,960  of  5,542.    Elapsed: 1:35:22.\n",
      "  Batch 3,000  of  5,542.    Elapsed: 1:36:39.\n",
      "  Batch 3,040  of  5,542.    Elapsed: 1:37:56.\n",
      "  Batch 3,080  of  5,542.    Elapsed: 1:39:13.\n",
      "  Batch 3,120  of  5,542.    Elapsed: 1:40:30.\n",
      "  Batch 3,160  of  5,542.    Elapsed: 1:41:47.\n",
      "  Batch 3,200  of  5,542.    Elapsed: 1:43:04.\n",
      "  Batch 3,240  of  5,542.    Elapsed: 1:44:21.\n",
      "  Batch 3,280  of  5,542.    Elapsed: 1:45:38.\n",
      "  Batch 3,320  of  5,542.    Elapsed: 1:46:56.\n",
      "  Batch 3,360  of  5,542.    Elapsed: 1:48:13.\n",
      "  Batch 3,400  of  5,542.    Elapsed: 1:49:30.\n",
      "  Batch 3,440  of  5,542.    Elapsed: 1:50:47.\n",
      "  Batch 3,480  of  5,542.    Elapsed: 1:52:04.\n",
      "  Batch 3,520  of  5,542.    Elapsed: 1:53:21.\n",
      "  Batch 3,560  of  5,542.    Elapsed: 1:54:38.\n",
      "  Batch 3,600  of  5,542.    Elapsed: 1:55:55.\n",
      "  Batch 3,640  of  5,542.    Elapsed: 1:57:12.\n",
      "  Batch 3,680  of  5,542.    Elapsed: 1:58:29.\n",
      "  Batch 3,720  of  5,542.    Elapsed: 1:59:46.\n",
      "  Batch 3,760  of  5,542.    Elapsed: 2:01:03.\n",
      "  Batch 3,800  of  5,542.    Elapsed: 2:02:20.\n",
      "  Batch 3,840  of  5,542.    Elapsed: 2:03:38.\n",
      "  Batch 3,880  of  5,542.    Elapsed: 2:04:55.\n",
      "  Batch 3,920  of  5,542.    Elapsed: 2:06:12.\n",
      "  Batch 3,960  of  5,542.    Elapsed: 2:07:29.\n",
      "  Batch 4,000  of  5,542.    Elapsed: 2:08:46.\n",
      "  Batch 4,040  of  5,542.    Elapsed: 2:10:03.\n",
      "  Batch 4,080  of  5,542.    Elapsed: 2:11:20.\n",
      "  Batch 4,120  of  5,542.    Elapsed: 2:12:37.\n",
      "  Batch 4,160  of  5,542.    Elapsed: 2:13:54.\n",
      "  Batch 4,200  of  5,542.    Elapsed: 2:15:12.\n",
      "  Batch 4,240  of  5,542.    Elapsed: 2:16:29.\n",
      "  Batch 4,280  of  5,542.    Elapsed: 2:17:46.\n",
      "  Batch 4,320  of  5,542.    Elapsed: 2:19:03.\n",
      "  Batch 4,360  of  5,542.    Elapsed: 2:20:20.\n",
      "  Batch 4,400  of  5,542.    Elapsed: 2:21:37.\n",
      "  Batch 4,440  of  5,542.    Elapsed: 2:22:54.\n",
      "  Batch 4,480  of  5,542.    Elapsed: 2:24:11.\n",
      "  Batch 4,520  of  5,542.    Elapsed: 2:25:28.\n",
      "  Batch 4,560  of  5,542.    Elapsed: 2:26:45.\n",
      "  Batch 4,600  of  5,542.    Elapsed: 2:28:03.\n",
      "  Batch 4,640  of  5,542.    Elapsed: 2:29:20.\n",
      "  Batch 4,680  of  5,542.    Elapsed: 2:30:37.\n",
      "  Batch 4,720  of  5,542.    Elapsed: 2:31:54.\n",
      "  Batch 4,760  of  5,542.    Elapsed: 2:33:11.\n",
      "  Batch 4,800  of  5,542.    Elapsed: 2:34:28.\n",
      "  Batch 4,840  of  5,542.    Elapsed: 2:35:45.\n",
      "  Batch 4,880  of  5,542.    Elapsed: 2:37:03.\n",
      "  Batch 4,920  of  5,542.    Elapsed: 2:38:19.\n",
      "  Batch 4,960  of  5,542.    Elapsed: 2:39:37.\n",
      "  Batch 5,000  of  5,542.    Elapsed: 2:40:54.\n",
      "  Batch 5,040  of  5,542.    Elapsed: 2:42:11.\n",
      "  Batch 5,080  of  5,542.    Elapsed: 2:43:28.\n",
      "  Batch 5,120  of  5,542.    Elapsed: 2:44:45.\n",
      "  Batch 5,160  of  5,542.    Elapsed: 2:46:02.\n",
      "  Batch 5,200  of  5,542.    Elapsed: 2:47:19.\n",
      "  Batch 5,240  of  5,542.    Elapsed: 2:48:36.\n",
      "  Batch 5,280  of  5,542.    Elapsed: 2:49:53.\n",
      "  Batch 5,320  of  5,542.    Elapsed: 2:51:10.\n",
      "  Batch 5,360  of  5,542.    Elapsed: 2:52:27.\n",
      "  Batch 5,400  of  5,542.    Elapsed: 2:53:44.\n",
      "  Batch 5,440  of  5,542.    Elapsed: 2:55:02.\n",
      "  Batch 5,480  of  5,542.    Elapsed: 2:56:18.\n",
      "  Batch 5,520  of  5,542.    Elapsed: 2:57:36.\n",
      "Running Validation...\n",
      "  Accuracy: 0.262\n",
      "  Validation Loss: 1.05\n",
      "  Validation took: 0:26:55\n",
      "======== Epoch 2 / 2 ========\n",
      "  Batch    40  of  5,542.    Elapsed: 0:01:17.\n",
      "  Batch    80  of  5,542.    Elapsed: 0:02:34.\n",
      "  Batch   120  of  5,542.    Elapsed: 0:03:51.\n",
      "  Batch   160  of  5,542.    Elapsed: 0:05:08.\n",
      "  Batch   200  of  5,542.    Elapsed: 0:06:25.\n",
      "  Batch   240  of  5,542.    Elapsed: 0:07:42.\n",
      "  Batch   280  of  5,542.    Elapsed: 0:08:59.\n",
      "  Batch   320  of  5,542.    Elapsed: 0:10:16.\n",
      "  Batch   360  of  5,542.    Elapsed: 0:11:33.\n",
      "  Batch   400  of  5,542.    Elapsed: 0:12:51.\n",
      "  Batch   440  of  5,542.    Elapsed: 0:14:08.\n",
      "  Batch   480  of  5,542.    Elapsed: 0:15:25.\n",
      "  Batch   520  of  5,542.    Elapsed: 0:16:42.\n",
      "  Batch   560  of  5,542.    Elapsed: 0:17:59.\n",
      "  Batch   600  of  5,542.    Elapsed: 0:19:16.\n",
      "  Batch   640  of  5,542.    Elapsed: 0:20:33.\n",
      "  Batch   680  of  5,542.    Elapsed: 0:21:51.\n",
      "  Batch   720  of  5,542.    Elapsed: 0:23:08.\n",
      "  Batch   760  of  5,542.    Elapsed: 0:24:25.\n",
      "  Batch   800  of  5,542.    Elapsed: 0:25:42.\n",
      "  Batch   840  of  5,542.    Elapsed: 0:26:59.\n",
      "  Batch   880  of  5,542.    Elapsed: 0:28:16.\n",
      "  Batch   920  of  5,542.    Elapsed: 0:29:33.\n",
      "  Batch   960  of  5,542.    Elapsed: 0:30:50.\n",
      "  Batch 1,000  of  5,542.    Elapsed: 0:32:08.\n",
      "  Batch 1,040  of  5,542.    Elapsed: 0:33:24.\n",
      "  Batch 1,080  of  5,542.    Elapsed: 0:34:42.\n",
      "  Batch 1,120  of  5,542.    Elapsed: 0:35:59.\n",
      "  Batch 1,160  of  5,542.    Elapsed: 0:37:16.\n",
      "  Batch 1,200  of  5,542.    Elapsed: 0:38:33.\n",
      "  Batch 1,240  of  5,542.    Elapsed: 0:39:50.\n",
      "  Batch 1,280  of  5,542.    Elapsed: 0:41:07.\n",
      "  Batch 1,320  of  5,542.    Elapsed: 0:42:24.\n",
      "  Batch 1,360  of  5,542.    Elapsed: 0:43:41.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,400  of  5,542.    Elapsed: 0:44:58.\n",
      "  Batch 1,440  of  5,542.    Elapsed: 0:46:15.\n",
      "  Batch 1,480  of  5,542.    Elapsed: 0:47:32.\n",
      "  Batch 1,520  of  5,542.    Elapsed: 0:48:49.\n",
      "  Batch 1,560  of  5,542.    Elapsed: 0:50:07.\n",
      "  Batch 1,600  of  5,542.    Elapsed: 0:51:24.\n",
      "  Batch 1,640  of  5,542.    Elapsed: 0:52:41.\n",
      "  Batch 1,680  of  5,542.    Elapsed: 0:53:58.\n",
      "  Batch 1,720  of  5,542.    Elapsed: 0:55:15.\n",
      "  Batch 1,760  of  5,542.    Elapsed: 0:56:32.\n",
      "  Batch 1,800  of  5,542.    Elapsed: 0:57:49.\n",
      "  Batch 1,840  of  5,542.    Elapsed: 0:59:06.\n",
      "  Batch 1,880  of  5,542.    Elapsed: 1:00:24.\n",
      "  Batch 1,920  of  5,542.    Elapsed: 1:02:42.\n",
      "  Batch 1,960  of  5,542.    Elapsed: 1:05:09.\n",
      "  Batch 2,000  of  5,542.    Elapsed: 1:06:42.\n",
      "  Batch 2,040  of  5,542.    Elapsed: 1:08:32.\n",
      "  Batch 2,080  of  5,542.    Elapsed: 1:09:53.\n",
      "  Batch 2,120  of  5,542.    Elapsed: 1:12:58.\n",
      "  Batch 2,160  of  5,542.    Elapsed: 1:14:21.\n",
      "  Batch 2,200  of  5,542.    Elapsed: 1:16:09.\n",
      "  Batch 2,240  of  5,542.    Elapsed: 1:18:04.\n",
      "  Batch 2,280  of  5,542.    Elapsed: 1:19:46.\n",
      "  Batch 2,320  of  5,542.    Elapsed: 1:21:29.\n",
      "  Batch 2,360  of  5,542.    Elapsed: 1:23:36.\n",
      "  Batch 2,400  of  5,542.    Elapsed: 1:25:20.\n",
      "  Batch 2,440  of  5,542.    Elapsed: 1:28:06.\n",
      "  Batch 2,480  of  5,542.    Elapsed: 1:30:24.\n",
      "  Batch 2,520  of  5,542.    Elapsed: 1:32:42.\n",
      "  Batch 2,560  of  5,542.    Elapsed: 1:34:21.\n",
      "  Batch 2,600  of  5,542.    Elapsed: 1:36:06.\n",
      "  Batch 2,640  of  5,542.    Elapsed: 1:37:51.\n",
      "  Batch 2,680  of  5,542.    Elapsed: 1:39:29.\n",
      "  Batch 2,720  of  5,542.    Elapsed: 1:41:06.\n",
      "  Batch 2,760  of  5,542.    Elapsed: 1:42:43.\n",
      "  Batch 2,800  of  5,542.    Elapsed: 1:44:21.\n",
      "  Batch 2,840  of  5,542.    Elapsed: 1:45:58.\n",
      "  Batch 2,880  of  5,542.    Elapsed: 1:47:35.\n",
      "  Batch 2,920  of  5,542.    Elapsed: 1:48:57.\n",
      "  Batch 2,960  of  5,542.    Elapsed: 1:50:14.\n",
      "  Batch 3,000  of  5,542.    Elapsed: 1:51:30.\n",
      "  Batch 3,040  of  5,542.    Elapsed: 1:52:53.\n",
      "  Batch 3,080  of  5,542.    Elapsed: 1:54:35.\n",
      "  Batch 3,120  of  5,542.    Elapsed: 1:56:37.\n",
      "  Batch 3,160  of  5,542.    Elapsed: 1:58:24.\n",
      "  Batch 3,200  of  5,542.    Elapsed: 2:00:34.\n",
      "  Batch 3,240  of  5,542.    Elapsed: 2:02:16.\n",
      "  Batch 3,280  of  5,542.    Elapsed: 2:04:27.\n",
      "  Batch 3,320  of  5,542.    Elapsed: 2:06:11.\n",
      "  Batch 3,360  of  5,542.    Elapsed: 2:07:51.\n",
      "  Batch 3,400  of  5,542.    Elapsed: 2:09:50.\n",
      "  Batch 3,440  of  5,542.    Elapsed: 2:11:45.\n",
      "  Batch 3,480  of  5,542.    Elapsed: 2:13:27.\n",
      "  Batch 3,520  of  5,542.    Elapsed: 2:15:04.\n",
      "  Batch 3,560  of  5,542.    Elapsed: 2:16:42.\n",
      "  Batch 3,600  of  5,542.    Elapsed: 2:18:19.\n",
      "  Batch 3,640  of  5,542.    Elapsed: 2:19:56.\n",
      "  Batch 3,680  of  5,542.    Elapsed: 2:21:33.\n",
      "  Batch 3,720  of  5,542.    Elapsed: 2:23:17.\n",
      "  Batch 3,760  of  5,542.    Elapsed: 2:25:54.\n",
      "  Batch 3,800  of  5,542.    Elapsed: 2:28:11.\n",
      "  Batch 3,840  of  5,542.    Elapsed: 2:29:59.\n",
      "  Batch 3,880  of  5,542.    Elapsed: 2:31:49.\n",
      "  Batch 3,920  of  5,542.    Elapsed: 2:33:42.\n",
      "  Batch 3,960  of  5,542.    Elapsed: 2:35:29.\n",
      "  Batch 4,000  of  5,542.    Elapsed: 2:37:20.\n",
      "  Batch 4,040  of  5,542.    Elapsed: 2:38:43.\n",
      "  Batch 4,080  of  5,542.    Elapsed: 2:40:00.\n",
      "  Batch 4,120  of  5,542.    Elapsed: 2:41:17.\n",
      "  Batch 4,160  of  5,542.    Elapsed: 2:42:34.\n",
      "  Batch 4,200  of  5,542.    Elapsed: 2:43:52.\n",
      "  Batch 4,240  of  5,542.    Elapsed: 2:45:09.\n",
      "  Batch 4,280  of  5,542.    Elapsed: 2:46:26.\n",
      "  Batch 4,320  of  5,542.    Elapsed: 2:47:43.\n",
      "  Batch 4,360  of  5,542.    Elapsed: 2:49:00.\n",
      "  Batch 4,400  of  5,542.    Elapsed: 2:50:17.\n",
      "  Batch 4,440  of  5,542.    Elapsed: 2:51:34.\n",
      "  Batch 4,480  of  5,542.    Elapsed: 2:52:51.\n",
      "  Batch 4,520  of  5,542.    Elapsed: 2:54:08.\n",
      "  Batch 4,560  of  5,542.    Elapsed: 2:55:25.\n",
      "  Batch 4,600  of  5,542.    Elapsed: 2:56:43.\n",
      "  Batch 4,640  of  5,542.    Elapsed: 2:58:00.\n",
      "  Batch 4,680  of  5,542.    Elapsed: 2:59:17.\n",
      "  Batch 4,720  of  5,542.    Elapsed: 3:00:34.\n",
      "  Batch 4,760  of  5,542.    Elapsed: 3:01:51.\n",
      "  Batch 4,800  of  5,542.    Elapsed: 3:03:08.\n",
      "  Batch 4,840  of  5,542.    Elapsed: 3:04:25.\n",
      "  Batch 4,880  of  5,542.    Elapsed: 3:05:42.\n",
      "  Batch 4,920  of  5,542.    Elapsed: 3:06:59.\n",
      "  Batch 4,960  of  5,542.    Elapsed: 3:08:17.\n",
      "  Batch 5,000  of  5,542.    Elapsed: 3:09:34.\n",
      "  Batch 5,040  of  5,542.    Elapsed: 3:10:51.\n",
      "  Batch 5,080  of  5,542.    Elapsed: 3:12:08.\n",
      "  Batch 5,120  of  5,542.    Elapsed: 3:13:25.\n",
      "  Batch 5,160  of  5,542.    Elapsed: 3:14:42.\n",
      "  Batch 5,200  of  5,542.    Elapsed: 3:15:59.\n",
      "  Batch 5,240  of  5,542.    Elapsed: 3:17:17.\n",
      "  Batch 5,280  of  5,542.    Elapsed: 3:18:37.\n",
      "  Batch 5,320  of  5,542.    Elapsed: 3:19:54.\n",
      "  Batch 5,360  of  5,542.    Elapsed: 3:21:11.\n",
      "  Batch 5,400  of  5,542.    Elapsed: 3:22:28.\n",
      "  Batch 5,440  of  5,542.    Elapsed: 3:23:45.\n",
      "  Batch 5,480  of  5,542.    Elapsed: 3:25:02.\n",
      "  Batch 5,520  of  5,542.    Elapsed: 3:26:19.\n",
      "Running Validation...\n",
      "  Accuracy: 0.262\n",
      "  Validation Loss: 1.04\n",
      "  Validation took: 0:26:54\n",
      "\n",
      "Training complete!\n",
      "Total training took 7:19:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        res = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = res['loss']\n",
    "        logits = res['logits']\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            res = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        loss = res['loss']\n",
    "        logits = res['logits']\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b41f3fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.05149</td>\n",
       "      <td>1.046846</td>\n",
       "      <td>0.262112</td>\n",
       "      <td>2:58:17</td>\n",
       "      <td>0:26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.04628</td>\n",
       "      <td>1.040827</td>\n",
       "      <td>0.262112</td>\n",
       "      <td>3:27:01</td>\n",
       "      <td>0:26:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1            1.05149     1.046846       0.262112       2:58:17         0:26:55\n",
       "2            1.04628     1.040827       0.262112       3:27:01         0:26:54"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
